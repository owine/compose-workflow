name: Deploy Docker Compose

on:
  workflow_call:
    inputs:
      args:
        description: "docker compose up -d arguments"
        required: false
        type: string
      stacks:
        description: "JSON array of stack names to deploy"
        required: true
        type: string
      webhook-url:
        description: "1Password reference to Discord webhook URL"
        required: true
        type: string
      repo-name:
        description: "Repository display name for notifications"
        required: true
        type: string
      target-ref:
        description: "Git reference to checkout on remote server"
        required: true
        type: string
      has-dockge:
        description: "Whether this deployment includes Dockge"
        required: false
        type: boolean
        default: false
      force-deploy:
        description: "Force deployment even if repository is already at target commit"
        required: false
        type: boolean
        default: false
      health-check-timeout:
        description: "Health check timeout in seconds (default: 180)"
        required: false
        type: number
        default: 180
      critical-services:
        description: "JSON array of critical service names that should trigger early exit on failure"
        required: false
        type: string
        default: '[]'

jobs:
  deploy:
    runs-on: ubuntu-24.04
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    timeout-minutes: 40  # Overall job timeout
    outputs:
      previous_sha: ${{ steps.backup.outputs.previous_sha }}
      deployment_needed: ${{ steps.backup.outputs.deployment_needed }}
      deploy_status: ${{ steps.deploy.outcome }}
      health_status: ${{ steps.health.outcome }}
      cleanup_status: ${{ steps.cleanup.outcome }}
      rollback_status: ${{ steps.rollback.outcome }}
      healthy_stacks: ${{ steps.health.outputs.healthy_stacks }}
      degraded_stacks: ${{ steps.health.outputs.degraded_stacks }}
      failed_stacks: ${{ steps.health.outputs.failed_stacks }}
      total_containers: ${{ steps.health.outputs.total_containers }}
      running_containers: ${{ steps.health.outputs.running_containers }}
      success_rate: ${{ steps.health.outputs.success_rate }}
    steps:
      - name: Validate and sanitize inputs
        run: |
          # Validate stacks parameter is valid JSON
          echo '${{ inputs.stacks }}' | jq -r '.[]' >/dev/null || {
            echo "::error::Invalid stacks JSON format: ${{ inputs.stacks }}"
            exit 1
          }

          # Validate stack names contain only safe characters
          INVALID_STACKS=""
          echo '${{ inputs.stacks }}' | jq -r '.[]' | while read -r stack; do
            if [[ ! "$stack" =~ ^[a-zA-Z0-9_-]+$ ]]; then
              echo "::error::Invalid stack name: $stack. Only alphanumeric, underscore, and hyphen allowed."
              exit 1
            fi
            # Check stack name length
            if [ ${#stack} -gt 50 ]; then
              echo "::error::Stack name too long: $stack (max 50 characters)"
              exit 1
            fi
          done

          # Validate target-ref format
          TARGET_REF="${{ inputs.target-ref }}"
          # Check if it's a valid commit SHA (7-40 hex chars) or branch/tag name
          if [[ "$TARGET_REF" =~ ^[a-fA-F0-9]{7,40}$ ]] || [[ "$TARGET_REF" =~ ^[a-zA-Z0-9_-]+$ ]] || [[ "$TARGET_REF" =~ ^[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+$ ]]; then
            echo "✅ Target-ref format valid: $TARGET_REF"
          else
            echo "::error::Invalid target-ref format: $TARGET_REF"
            echo "::error::Expected: commit SHA (7-40 hex chars) or branch/tag name"
            exit 1
          fi

          # Validate and sanitize compose args
          COMPOSE_ARGS="${{ inputs.args }}"
          if [[ -n "$COMPOSE_ARGS" ]]; then
            # Check for dangerous characters and patterns
            if [[ "$COMPOSE_ARGS" =~ [\;\&\|\`\$\\] ]]; then
              echo "::error::Compose args contain potentially dangerous characters: $COMPOSE_ARGS"
              echo "::error::Prohibited characters: ; & | \` $ \\"
              exit 1
            fi
            # Check for suspicious patterns
            if [[ "$COMPOSE_ARGS" =~ (rm|kill|shutdown|reboot|format|dd|\>|\<|sudo|su) ]]; then
              echo "::error::Compose args contain prohibited commands: $COMPOSE_ARGS"
              exit 1
            fi
            # Validate against known docker compose options - allow hyphens, spaces, and equals for arguments
            if [[ "$COMPOSE_ARGS" =~ ^[a-zA-Z0-9[:space:]_=.-]+$ ]]; then
              echo "✅ Compose args format valid: $COMPOSE_ARGS"
            else
              echo "::error::Compose args contain invalid characters: $COMPOSE_ARGS"
              exit 1
            fi
          fi

          # Validate webhook URL format
          WEBHOOK_URL="${{ inputs.webhook-url }}"
          if [[ ! "$WEBHOOK_URL" =~ ^op://[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+$ ]]; then
            echo "::error::Invalid webhook URL format: $WEBHOOK_URL"
            echo "::error::Expected format: op://vault/item/field"
            exit 1
          fi

          # Validate repo name
          REPO_NAME="${{ inputs.repo-name }}"
          if [[ ! "$REPO_NAME" =~ ^[a-zA-Z0-9_-]+$ ]] || [ ${#REPO_NAME} -gt 100 ]; then
            echo "::error::Invalid repo name: $REPO_NAME"
            echo "::error::Must be alphanumeric with hyphens/underscores, max 100 chars"
            exit 1
          fi

          echo "✅ All input validation passed"

      - name: Setup retry mechanism
        run: |
          # Create retry function for bash commands
          cat > /tmp/retry.sh << 'EOF'
          #!/bin/bash
          retry() {
            local max_attempts=$1
            local delay=$2
            local command="${@:3}"
            local attempt=1

            while [ $attempt -le $max_attempts ]; do
              echo "Attempt $attempt of $max_attempts: $command"
              if eval "$command"; then
                echo "✅ Command succeeded on attempt $attempt"
                return 0
              else
                echo "❌ Command failed on attempt $attempt"
                if [ $attempt -lt $max_attempts ]; then
                  echo "⏳ Waiting ${delay}s before retry..."
                  sleep $delay
                  delay=$((delay * 2))  # Exponential backoff
                fi
                attempt=$((attempt + 1))
              fi
            done

            echo "💥 Command failed after $max_attempts attempts"
            return 1
          }

          # Create SSH retry function with specific error handling
          ssh_retry() {
            local max_attempts=$1
            local delay=$2
            local ssh_cmd="${@:3}"
            local attempt=1

            while [ $attempt -le $max_attempts ]; do
              echo "SSH Attempt $attempt of $max_attempts"
              if eval "$ssh_cmd"; then
                echo "✅ SSH command succeeded on attempt $attempt"
                return 0
              else
                local exit_code=$?
                echo "❌ SSH command failed on attempt $attempt (exit code: $exit_code)"

                # Check for specific SSH errors
                case $exit_code in
                  255) echo "SSH connection error - network/auth issue" ;;
                  1) echo "General SSH error" ;;
                  *) echo "Unknown error code: $exit_code" ;;
                esac

                if [ $attempt -lt $max_attempts ]; then
                  echo "⏳ Waiting ${delay}s before SSH retry..."
                  sleep $delay
                fi
                attempt=$((attempt + 1))
              fi
            done

            echo "💥 SSH command failed after $max_attempts attempts"
            return 1
          }
          EOF
          chmod +x /tmp/retry.sh

      - name: Cache Tailscale state
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4.3.0
        with:
          path: |
            ~/.cache/tailscale
            /var/lib/tailscale
          key: tailscale-${{ runner.os }}-${{ github.repository_owner }}-${{ github.run_number }}
          restore-keys: |
            tailscale-${{ runner.os }}-${{ github.repository_owner }}-
            tailscale-${{ runner.os }}-

      - name: Cache deployment tools
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4.3.0
        with:
          path: |
            ~/.cache/pip
            ~/.cache/docker
            ~/.ssh
          key: deploy-tools-${{ runner.os }}-v1
          restore-keys: |
            deploy-tools-${{ runner.os }}-

      - name: Configure 1Password Service Account
        uses: 1password/load-secrets-action/configure@13f58eec611f8e5db52ec16247f58c508398f3e6  # v3.0.0
        with:
          service-account-token: ${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}

      - name: Load Tailscale credentials
        id: load-tailscale-credentials
        uses: 1password/load-secrets-action@13f58eec611f8e5db52ec16247f58c508398f3e6  # v3.0.0
        with:
          unset-previous: true
        env:
          TAILSCALE_OAUTH_CLIENT_ID: "op://Docker/tailscale-oauth/client_id"
          TAILSCALE_OAUTH_SECRET: "op://Docker/tailscale-oauth/secret"

      - name: Connect to Tailnet
        uses: tailscale/github-action@6cae46e2d796f265265cfcf628b72a32b4d7cade  # v3.3.0
        with:
          oauth-client-id: ${{ steps.load-tailscale-credentials.outputs.TAILSCALE_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ steps.load-tailscale-credentials.outputs.TAILSCALE_OAUTH_SECRET }}
          tags: tag:ci
          targets: ${{ secrets.SSH_HOST }}
          use-cache: true
          version: latest

      - name: Unload Tailscale credentials
        uses: 1password/load-secrets-action@13f58eec611f8e5db52ec16247f58c508398f3e6  # v3.0.0
        with:
          unset-previous: true

      - name: Optimize SSH connections
        run: |
          # Configure SSH connection multiplexing for better performance
          mkdir -p ~/.ssh
          cat >> ~/.ssh/config << EOF
          Host deployment-server
            HostName ${{ secrets.SSH_HOST }}
            User ${{ secrets.SSH_USER }}
            ControlMaster auto
            ControlPath ~/.ssh/sockets/%r@%h:%p
            ControlPersist 300
            ServerAliveInterval 30
            ServerAliveCountMax 3
            Compression yes
            TCPKeepAlive yes
          EOF

          # Create control socket directory and pre-establish SSH connection
          mkdir -p ~/.ssh/sockets
          echo "🔗 Pre-establishing SSH connection for multiplexing..."
          ssh -o "StrictHostKeyChecking no" deployment-server -O check 2>/dev/null || \
          ssh -o "StrictHostKeyChecking no" deployment-server -O forward -N &

          # Give the connection a moment to establish
          sleep 2

          echo "✅ SSH connection optimization configured"

      - name: Store current deployment for rollback
        id: backup
        run: |
          echo "::group::Preparing deployment backup"
          # Use retry mechanism for SSH connection
          source /tmp/retry.sh

          # Get current deployment SHA with error handling
          echo "🔍 Checking current deployment SHA..."
          if CURRENT_SHA=$(ssh_retry 3 5 "ssh -o 'StrictHostKeyChecking no' ${{ secrets.SSH_USER }}@${{ secrets.SSH_HOST }} 'cd /opt/compose && git rev-parse HEAD 2>/dev/null'"); then
            # Validate SHA format
            if [[ "$CURRENT_SHA" =~ ^[a-fA-F0-9]{40}$ ]]; then
              echo "✅ Current deployed SHA: $CURRENT_SHA"
            else
              echo "⚠️ Invalid SHA format from server: $CURRENT_SHA"
              CURRENT_SHA="unknown"
            fi
          else
            echo "⚠️ Could not retrieve current deployment SHA - assuming first deployment"
            CURRENT_SHA="unknown"
          fi

          TARGET_REF="${{ inputs.target-ref }}"
          echo "🎯 Target deployment ref: $TARGET_REF"

          # Resolve target ref to SHA if it's not already a SHA
          if [[ "$TARGET_REF" =~ ^[a-fA-F0-9]{40}$ ]]; then
            TARGET_SHA="$TARGET_REF"
            echo "✅ Target ref is already a full SHA"
          elif [[ "$TARGET_REF" =~ ^[a-fA-F0-9]{7,39}$ ]]; then
            TARGET_SHA="$TARGET_REF"
            echo "✅ Target ref is a short SHA, will resolve on server"
          else
            TARGET_SHA="$TARGET_REF"
            echo "✅ Target ref is a branch/tag name, will resolve on server"
          fi

          # Set outputs with proper validation
          echo "previous_sha=${CURRENT_SHA}" >> $GITHUB_OUTPUT

          if [ "$CURRENT_SHA" = "$TARGET_SHA" ] && [ "${{ inputs.force-deploy }}" != "true" ]; then
            echo "⚠️ Repository is already at target commit - no deployment needed"
            echo "deployment_needed=false" >> $GITHUB_OUTPUT
          elif [ "$CURRENT_SHA" = "$TARGET_SHA" ] && [ "${{ inputs.force-deploy }}" = "true" ]; then
            echo "🔄 Force deployment requested - proceeding despite same commit"
            echo "deployment_needed=true" >> $GITHUB_OUTPUT
          else
            echo "✅ Deployment needed - proceeding with update"
            echo "deployment_needed=true" >> $GITHUB_OUTPUT
          fi
          echo "::endgroup::"

      - name: Deploy All Stacks
        id: deploy
        if: steps.backup.outputs.deployment_needed == 'true'
        continue-on-error: true
        run: |
          echo "::group::Deploying all stacks"

          # Source retry functions
          source /tmp/retry.sh

          # Set error handling
          set -e
          trap 'echo "❌ Deployment failed at line $LINENO"' ERR

          # Parse inputs outside SSH context
          STACKS="${{ join(fromJson(inputs.stacks), ' ') }}"
          OP_TOKEN="${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}"
          HAS_DOCKGE="${{ inputs.has-dockge }}"
          TARGET_REF="${{ inputs.target-ref }}"
          COMPOSE_ARGS="${{ inputs.args || '' }}"
          HEALTH_CHECK_TIMEOUT="${{ inputs.health-check-timeout }}"
          CRITICAL_SERVICES='${{ inputs.critical-services }}'


          # Use retry mechanism and optimized deployment
          ssh_retry 3 10 "ssh -o \"StrictHostKeyChecking no\" deployment-server /bin/bash -s $STACKS \"$HAS_DOCKGE\" \"$TARGET_REF\" \"$COMPOSE_ARGS\"" << 'EOF'
            set -e

            # Performance optimizations
            export DOCKER_BUILDKIT=1
            export COMPOSE_DOCKER_CLI_BUILD=1

            # Enable parallel image pulls
            export COMPOSE_PARALLEL_LIMIT=8

            # Get arguments passed to script (excluding sensitive OP_TOKEN)
            # Arguments: stack1 stack2 stack3 ... HAS_DOCKGE TARGET_REF [COMPOSE_ARGS]
            # COMPOSE_ARGS might be empty, so we need to handle variable arg count

            TOTAL_ARGS=$#

            # Find HAS_DOCKGE by looking for 'true' or 'false' in the args
            HAS_DOCKGE=""
            TARGET_REF=""
            COMPOSE_ARGS=""

            # The last few args should be: HAS_DOCKGE TARGET_REF [COMPOSE_ARGS]
            # HAS_DOCKGE is always 'true' or 'false'
            # TARGET_REF is a commit SHA (starts with letter/number)
            # COMPOSE_ARGS is optional and could be empty

            for i in $(seq 1 $TOTAL_ARGS); do
              ARG="${!i}"
              if [ "$ARG" = "true" ] || [ "$ARG" = "false" ]; then
                HAS_DOCKGE="$ARG"
                TARGET_REF="${@:$((i+1)):1}"
                if [ $((i+2)) -le $TOTAL_ARGS ]; then
                  COMPOSE_ARGS="${@:$((i+2)):1}"
                fi
                # All args before this position are stack names
                STACKS="${@:1:$((i-1))}"
                break
              fi
            done


            # Set OP_TOKEN via environment (passed separately)
            export OP_SERVICE_ACCOUNT_TOKEN="${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}"

            if [ "$HAS_DOCKGE" = "true" ]; then
              echo "🚀 Deploying Dockge..."
              cd /opt/dockge
              op run --env-file=/opt/compose/compose.env -- docker compose pull
              op run --env-file=/opt/compose/compose.env -- docker compose up -d --remove-orphans $COMPOSE_ARGS
              echo "✅ Dockge deployed successfully"
            fi

            echo "Updating repository to $TARGET_REF..."
            git -C /opt/compose/ fetch
            git -C /opt/compose/ checkout $TARGET_REF

            # Function to deploy a single stack
            deploy_stack() {
              local STACK=$1
              local LOGFILE="/tmp/deploy_${STACK}.log"
              {
                echo "🚀 Deploying $STACK..."
                cd /opt/compose/$STACK

                echo "  Pulling images for $STACK..."
                if ! op run --env-file=/opt/compose/compose.env -- docker compose pull; then
                  echo "❌ Failed to pull images for $STACK"
                  return 1
                fi

                echo "  Starting services for $STACK..."
                if ! op run --env-file=/opt/compose/compose.env -- docker compose up -d --remove-orphans $COMPOSE_ARGS; then
                  echo "❌ Failed to start services for $STACK"
                  return 1
                fi

                echo "✅ $STACK deployed successfully"
                return 0
              } > "$LOGFILE" 2>&1

              return $?
            }

            # Cleanup function for deploy logs
            cleanup_deploy_logs() {
              for STACK in $STACKS; do
                rm -f "/tmp/deploy_${STACK}.log" 2>/dev/null
              done
            }

            # Pre-deployment validation function
            validate_all_stacks() {
              echo "🔍 Pre-deployment validation of all stacks..."
              local validation_failed=false

              for STACK in $STACKS; do
                echo "  Validating $STACK..."

                # Check if stack directory exists
                if [ ! -d "/opt/compose/$STACK" ]; then
                  echo "❌ $STACK: Directory /opt/compose/$STACK not found"
                  validation_failed=true
                  continue
                fi

                cd "/opt/compose/$STACK" || {
                  echo "❌ $STACK: Cannot access directory"
                  validation_failed=true
                  continue
                }

                # Check if compose.yaml exists
                if [ ! -f "compose.yaml" ]; then
                  echo "❌ $STACK: compose.yaml not found"
                  validation_failed=true
                  continue
                fi

                # Validate 1Password environment access and Docker Compose config
                if ! timeout 30 op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml config --services >/dev/null 2>&1; then
                  echo "❌ $STACK: Environment validation failed (1Password or compose config error)"
                  validation_failed=true
                  continue
                fi

                # Quick syntax validation
                if ! timeout 15 op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml config --quiet 2>/dev/null; then
                  echo "❌ $STACK: Docker Compose syntax validation failed"
                  validation_failed=true
                  continue
                fi

                echo "✅ $STACK: Pre-deployment validation passed"
              done

              if [ "$validation_failed" = true ]; then
                echo "❌ Pre-deployment validation failed for one or more stacks"
                echo "   Stopping deployment to prevent extended failures"
                return 1
              fi

              echo "✅ All stacks passed pre-deployment validation"
              return 0
            }

            # Run pre-deployment validation
            if ! validate_all_stacks; then
              echo "DEPLOYMENT_STATUS=failed_validation" >> "$GITHUB_OUTPUT"
              exit 1
            fi

            # Set trap for cleanup on exit
            trap cleanup_deploy_logs EXIT

            # Start all deployments in parallel
            echo "🚀 Starting parallel deployment of all stacks..."
            PIDS=""

            # Simple approach - use for loop directly with unquoted variable
            for STACK in $STACKS; do
              echo "🚀 Deploying $STACK..."
              deploy_stack "$STACK" &
              PIDS="$PIDS $!"
              echo "Started deployment of $STACK (PID: $!)"
            done

            # Wait for all deployments and collect results
            echo "⏳ Waiting for all deployments to complete..."
            FAILED_STACKS=""

            # Enhanced parallel job monitoring with better error propagation
            echo "⏳ Monitoring parallel deployments..."
            DEPLOYED_STACKS=""
            SUCCESSFUL_STACKS=""
            DEPLOYMENT_ERRORS=""

            # Wait for jobs individually to capture exit codes
            for PID in $PIDS; do
              if wait "$PID"; then
                echo "✅ Deployment process $PID completed successfully"
              else
                EXIT_CODE=$?
                echo "❌ Deployment process $PID failed with exit code $EXIT_CODE"
                DEPLOYMENT_ERRORS="$DEPLOYMENT_ERRORS PID:$PID:$EXIT_CODE"
              fi
            done

            # Enhanced result analysis from log files
            for STACK in $STACKS; do
              if [ -f "/tmp/deploy_${STACK}.log" ]; then
                DEPLOYED_STACKS="$DEPLOYED_STACKS $STACK"

                # More comprehensive error detection
                if grep -q "❌.*$STACK\|CRITICAL.*$STACK\|Failed.*$STACK\|Error.*$STACK" "/tmp/deploy_${STACK}.log"; then
                  FAILED_STACKS="$FAILED_STACKS $STACK"
                  # Extract specific error for reporting
                  STACK_ERROR=$(grep -E "❌.*$STACK|CRITICAL.*$STACK|Failed.*$STACK|Error.*$STACK" "/tmp/deploy_${STACK}.log" | head -1)
                  echo "🔍 $STACK Error: $STACK_ERROR"
                elif grep -q "✅.*$STACK\|Successfully.*$STACK" "/tmp/deploy_${STACK}.log"; then
                  SUCCESSFUL_STACKS="$SUCCESSFUL_STACKS $STACK"
                fi
              else
                echo "⚠️ $STACK: No deployment log found - possible early failure"
                FAILED_STACKS="$FAILED_STACKS $STACK"
              fi
            done

            # Summary of deployment results
            echo ""
            echo "📊 Deployment Summary:"
            echo "  Successful: $(echo $SUCCESSFUL_STACKS | wc -w | tr -d ' ') stacks"
            echo "  Failed: $(echo $FAILED_STACKS | wc -w | tr -d ' ') stacks"
            if [ -n "$DEPLOYMENT_ERRORS" ]; then
              echo "  Process errors: $DEPLOYMENT_ERRORS"
            fi

            # Display deployment logs with enhanced formatting
            echo ""
            echo "📋 Detailed Deployment Results:"
            echo "════════════════════════════════════════════════════════════════"
            for STACK in $STACKS; do
              if [ -f "/tmp/deploy_${STACK}.log" ]; then
                echo ""
                echo "🔸 STACK: $STACK"
                echo "────────────────────────────────────────────────────────────────"
                cat "/tmp/deploy_${STACK}.log"
                echo "────────────────────────────────────────────────────────────────"
              else
                echo ""
                echo "🔸 STACK: $STACK"
                echo "────────────────────────────────────────────────────────────────"
                echo "⚠️  No deployment log found for $STACK"
                echo "────────────────────────────────────────────────────────────────"
              fi
            done
            echo "════════════════════════════════════════════════════════════════"

            # Check if any deployments failed
            if [ -z "$STACKS" ]; then
              echo "💥 No stacks to deploy - STACKS variable is empty!"
              exit 1
            elif [ -z "$DEPLOYED_STACKS" ]; then
              echo "💥 No stacks were actually deployed - check stack discovery!"
              exit 1
            elif [ -n "$FAILED_STACKS" ]; then
              echo "💥 Deployments failed for:$FAILED_STACKS"
              exit 1
            fi

            echo "🎉 All stacks deployed successfully in parallel!"
          EOF
          echo "::endgroup::"

      - name: Health Check All Services
        id: health
        if: steps.backup.outputs.deployment_needed == 'true' && steps.deploy.outcome == 'success'
        run: |
          echo "::group::Health checking all services"

          # Source retry functions
          source /tmp/retry.sh

          # Set error handling
          set -e
          trap 'echo "❌ Health check failed at line $LINENO"' ERR

          # Parse inputs outside SSH context
          STACKS="${{ join(fromJSON(inputs.stacks), ' ') }}"
          OP_TOKEN="${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}"
          HAS_DOCKGE="${{ inputs.has-dockge }}"

          # Execute health check and capture structured output
          # Use retry mechanism for health check
          HEALTH_RESULT=$(ssh_retry 3 5 "ssh -o \"StrictHostKeyChecking no\" ${{ secrets.SSH_USER }}@${{ secrets.SSH_HOST }} /bin/bash -s $STACKS \"$HAS_DOCKGE\"" << 'EOF'
            set -e

            # Get arguments passed to script (excluding sensitive OP_TOKEN)
            TOTAL_ARGS=$#

            # Find HAS_DOCKGE by looking for 'true' or 'false' in the args
            HAS_DOCKGE=""

            for i in $(seq 1 $TOTAL_ARGS); do
              ARG="${!i}"
              if [ "$ARG" = "true" ] || [ "$ARG" = "false" ]; then
                HAS_DOCKGE="$ARG"
                # All args before this position are stack names
                STACKS="${@:1:$((i-1))}"
                break
              fi
            done

            # Set OP_TOKEN via environment (passed separately)
            export OP_SERVICE_ACCOUNT_TOKEN="${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}"

            # Enhanced health check with exponential backoff
            echo "🔍 Starting enhanced health check with exponential backoff..."

            # Health check function with retry logic
            # Pre-validate 1Password environment file access
            validate_environment() {
              echo "🔐 Validating 1Password environment access..."
              if ! op run --env-file=/opt/compose/compose.env -- echo "Environment validation successful" >/dev/null 2>&1; then
                echo "❌ CRITICAL: Failed to load environment variables from 1Password"
                echo "   This indicates missing 1Password items or authentication issues"
                echo "   Stopping deployment to prevent extended failure loops"
                return 1
              fi
              echo "✅ 1Password environment validation passed"
              return 0
            }

            health_check_with_retry() {
              local stack=$1
              # Use configurable timeout with fallback to defaults
              local timeout_seconds=${HEALTH_CHECK_TIMEOUT:-180}
              local max_attempts=4  # Reduced from 6
              local wait_time=3     # Reduced initial wait from 5s
              local attempt=1
              local fast_fail_threshold=2  # Fast fail after 2 attempts if no progress
              local start_time=$(date +%s)

              echo "🕰️ Health check timeout configured: ${timeout_seconds}s"

              echo "🔍 Health checking $stack with optimized retry logic..."

              # Pre-validate environment access to fail fast
              if ! validate_environment; then
                return 1
              fi

              cd "/opt/compose/$stack" || {
                echo "❌ $stack: Directory not found"
                return 1
              }

              local previous_running=0
              local no_progress_count=0

              while [ $attempt -le $max_attempts ]; do
                echo "  Attempt $attempt/$max_attempts for $stack (wait: ${wait_time}s)"

                # Get container status with error handling
                local running_count total_count exited_count restarting_count

                # Check overall timeout
                local current_time=$(date +%s)
                local elapsed=$((current_time - start_time))
                if [ $elapsed -gt $timeout_seconds ]; then
                  echo "❌ $stack: Health check timed out after ${elapsed}s (limit: ${timeout_seconds}s)"
                  return 1
                fi

                # Use timeout to prevent hanging on 1Password errors
                if ! timeout 30 op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml config --services >/dev/null 2>&1; then
                  echo "❌ $stack: Environment or compose config error (timeout/1Password failure)"
                  return 1
                fi

                running_count=$(timeout 15 op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml ps --services --filter "status=running" 2>/dev/null | grep -E '^[a-zA-Z0-9_-]+$' | wc -l | tr -d " " || echo "0")
                total_count=$(timeout 15 op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml config --services 2>/dev/null | grep -E '^[a-zA-Z0-9_-]+$' | wc -l | tr -d " " || echo "0")
                exited_count=$(timeout 15 op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml ps --services --filter "status=exited" 2>/dev/null | grep -E '^[a-zA-Z0-9_-]+$' | wc -l | tr -d " " || echo "0")
                restarting_count=$(timeout 15 op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml ps --services --filter "status=restarting" 2>/dev/null | grep -E '^[a-zA-Z0-9_-]+$' | wc -l | tr -d " " || echo "0")

                echo "  $stack status: $running_count/$total_count running, $exited_count exited, $restarting_count restarting"

                # Fast fail logic: if no progress after 2 attempts and containers are failing
                if [ $attempt -ge $fast_fail_threshold ] && [ "$running_count" -eq "$previous_running" ] && [ "$exited_count" -gt 0 ]; then
                  no_progress_count=$((no_progress_count + 1))
                  if [ $no_progress_count -ge 2 ]; then
                    echo "❌ $stack: Fast fail - no progress and containers failing (attempt $attempt)"
                    return 1
                  fi
                else
                  no_progress_count=0
                fi

                # Success condition
                if [ "$running_count" -eq "$total_count" ] && [ "$total_count" -gt 0 ]; then
                  echo "✅ $stack: All $running_count services healthy"
                  return 0
                # Degraded but stable condition
                elif [ "$running_count" -gt 0 ] && [ "$restarting_count" -eq 0 ] && [ "$exited_count" -eq 0 ]; then
                  echo "⚠️ $stack: $running_count/$total_count running (degraded but stable)"
                  return 2  # Degraded but acceptable
                # Final attempt failure
                elif [ $attempt -eq $max_attempts ]; then
                  echo "❌ $stack: Failed after $max_attempts attempts ($running_count/$total_count running)"
                  return 1
                # Continue with shorter, smarter waits
                else
                  echo "  $stack: Not ready yet, waiting ${wait_time}s..."
                  sleep $wait_time
                  # More aggressive exponential backoff with lower cap
                  wait_time=$((wait_time * 2))
                  if [ $wait_time -gt 20 ]; then  # Cap at 20s instead of 120s
                    wait_time=20
                  fi
                fi

                previous_running=$running_count
                attempt=$((attempt + 1))
              done

              echo "❌ $stack: Health check failed after $max_attempts attempts"
              return 1
            }

            FAILED_STACKS=""
            DEGRADED_STACKS=""
            HEALTHY_STACKS=""
            TOTAL_CONTAINERS=0
            RUNNING_CONTAINERS=0

            if [ "$HAS_DOCKGE" = "true" ]; then
              echo "🔍 Health checking Dockge..."
              cd /opt/dockge
              DOCKGE_RUNNING=$(op run --env-file=/opt/compose/compose.env -- docker compose ps --services --filter "status=running" | wc -l | tr -d " ")
              DOCKGE_TOTAL=$(op run --env-file=/opt/compose/compose.env -- docker compose ps --services | wc -l | tr -d " ")
              TOTAL_CONTAINERS=$((TOTAL_CONTAINERS + DOCKGE_TOTAL))
              RUNNING_CONTAINERS=$((RUNNING_CONTAINERS + DOCKGE_RUNNING))

              if [ "$DOCKGE_RUNNING" -eq 0 ]; then
                echo "❌ Dockge: 0/$DOCKGE_TOTAL services running"
                FAILED_STACKS="$FAILED_STACKS dockge"
              elif [ "$DOCKGE_RUNNING" -lt "$DOCKGE_TOTAL" ]; then
                echo "⚠️ Dockge: $DOCKGE_RUNNING/$DOCKGE_TOTAL services running (degraded)"
                DEGRADED_STACKS="$DEGRADED_STACKS dockge"
              else
                echo "✅ Dockge: All $DOCKGE_RUNNING services healthy"
                HEALTHY_STACKS="$HEALTHY_STACKS dockge"
              fi
            fi

            # Function to perform comprehensive health check on a stack
            health_check_stack() {
              local STACK=$1
              local LOGFILE="/tmp/health_${STACK}.log"
              {
                echo "🔍 Health checking $STACK..."
                cd /opt/compose/$STACK

                # Get container status information using simpler approach
                RUNNING_COUNT=$(op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml ps --services --filter "status=running" 2>/dev/null | grep -v "^time=" | grep -v "^$" | wc -l | tr -d " " || echo "0")
                TOTAL_COUNT=$(op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml config --services 2>/dev/null | grep -v "^time=" | grep -v "^$" | wc -l | tr -d " " || echo "0")
                EXITED_COUNT=$(op run --env-file=/opt/compose/compose.env -- docker compose -f compose.yaml ps --services --filter "status=exited" 2>/dev/null | grep -v "^time=" | grep -v "^$" | wc -l | tr -d " " || echo "0")

                # Set HEALTHY_COUNT to same as RUNNING_COUNT for simplicity
                HEALTHY_COUNT=$RUNNING_COUNT

                # For simplicity, assume no restart issues or resource monitoring for now
                RESTART_ISSUES=0
                RESOURCE_INFO=""

                # Determine stack health status
                if [ "$HEALTHY_COUNT" -eq 0 ]; then
                  echo "❌ $STACK: No services running ($EXITED_COUNT exited)"
                  [ -n "$RESOURCE_INFO" ] && echo "   Resources: $RESOURCE_INFO"
                  return 1
                elif [ "$HEALTHY_COUNT" -lt "$TOTAL_COUNT" ]; then
                  echo "⚠️  $STACK: $HEALTHY_COUNT/$TOTAL_COUNT services running"
                  [ "$EXITED_COUNT" -gt 0 ] && echo "   $EXITED_COUNT services exited"
                  [ "$RESTART_ISSUES" -gt 0 ] && echo "   ⚠️ $RESTART_ISSUES services restarting"
                  [ -n "$RESOURCE_INFO" ] && echo "   Resources: $RESOURCE_INFO"
                  return 2  # Degraded state
                else
                  echo "✅ $STACK: All $HEALTHY_COUNT services healthy"
                  [ "$RESTART_ISSUES" -gt 0 ] && echo "   ⚠️ $RESTART_ISSUES services had recent restarts"
                  [ -n "$RESOURCE_INFO" ] && echo "   Resources: $RESOURCE_INFO"
                  return 0
                fi
              } > "$LOGFILE" 2>&1

              return $?
            }

            # Cleanup function for health logs
            cleanup_health_logs() {
              for STACK in $STACKS; do
                rm -f "/tmp/health_${STACK}.log" 2>/dev/null
              done
            }

            # Set trap for cleanup on exit
            trap cleanup_health_logs EXIT

            # Parse critical services list
            CRITICAL_SERVICES_ARRAY=""
            if [ -n "$CRITICAL_SERVICES" ] && [ "$CRITICAL_SERVICES" != "[]" ]; then
              # Convert JSON array to bash array (simple approach)
              CRITICAL_SERVICES_ARRAY=$(echo "$CRITICAL_SERVICES" | sed 's/\[\|\]\|"//g' | tr ',' ' ')
              echo "🚨 Critical services configured: $CRITICAL_SERVICES_ARRAY"
            fi

            # Function to check if a stack/service is critical
            is_critical_service() {
              local service=$1
              for critical in $CRITICAL_SERVICES_ARRAY; do
                if [ "$service" = "$critical" ]; then
                  return 0
                fi
              done
              return 1
            }

            # Enhanced health checks with sequential retry logic and early exit
            echo "🔍 Starting enhanced health checks with retry logic..."
            CRITICAL_FAILURE=false

            # Check each stack with the new enhanced health check
            for STACK in $STACKS; do
              echo ""
              echo "🔍 Checking stack: $STACK"

              health_check_with_retry "$STACK"
              HEALTH_RESULT=$?

              case $HEALTH_RESULT in
                0)
                  echo "✅ $STACK: Healthy"
                  HEALTHY_STACKS="$HEALTHY_STACKS $STACK"
                  ;;
                2)
                  echo "⚠️ $STACK: Degraded but stable"
                  DEGRADED_STACKS="$DEGRADED_STACKS $STACK"
                  # Check if degraded stack is critical
                  if is_critical_service "$STACK"; then
                    echo "🚨 CRITICAL SERVICE DEGRADED: $STACK"
                    echo "   Continuing monitoring but flagging for attention"
                  fi
                  ;;
                *)
                  echo "❌ $STACK: Failed health check"
                  FAILED_STACKS="$FAILED_STACKS $STACK"
                  # Check if failed stack is critical - trigger early exit
                  if is_critical_service "$STACK"; then
                    echo "🚨 CRITICAL SERVICE FAILURE: $STACK"
                    echo "   This is a critical service failure - triggering early exit"
                    echo "   Remaining stacks will not be health checked"
                    CRITICAL_FAILURE=true
                    break
                  fi
                  ;;
              esac
            done

            # Handle critical service failure
            if [ "$CRITICAL_FAILURE" = true ]; then
              echo ""
              echo "❌ CRITICAL SERVICE FAILURE DETECTED"
              echo "   Deployment marked as failed due to critical service failure"
              echo "   Health check terminated early to prevent extended failure cycles"
              # Set outputs for early termination
              echo "health_status=failed_critical" >> "$GITHUB_OUTPUT"
              echo "failed_stacks=$FAILED_STACKS" >> "$GITHUB_OUTPUT"
              echo "healthy_stacks=$HEALTHY_STACKS" >> "$GITHUB_OUTPUT"
              echo "degraded_stacks=$DEGRADED_STACKS" >> "$GITHUB_OUTPUT"
              exit 1
            fi

            # Calculate container statistics from all stacks
            for STACK in $STACKS; do
              STACK_CONTAINERS=$(cd /opt/compose/$STACK && op run --env-file=/opt/compose/compose.env -- docker compose config --services 2>/dev/null | wc -l | tr -d " " || echo "0")
              STACK_RUNNING=$(cd /opt/compose/$STACK && op run --env-file=/opt/compose/compose.env -- docker compose ps --services --filter "status=running" 2>/dev/null | wc -l | tr -d " " || echo "0")
              TOTAL_CONTAINERS=$((TOTAL_CONTAINERS + STACK_CONTAINERS))
              RUNNING_CONTAINERS=$((RUNNING_CONTAINERS + STACK_RUNNING))
            done

            # Display comprehensive health check results
            echo ""
            echo "📊 Health Check Summary:"
            echo "════════════════════════"
            echo "Total Services: $TOTAL_CONTAINERS"
            echo "Running Services: $RUNNING_CONTAINERS"
            echo "Success Rate: $(( RUNNING_CONTAINERS * 100 / TOTAL_CONTAINERS ))%"
            echo ""

            # Display results by category
            [ -n "$HEALTHY_STACKS" ] && echo "✅ Healthy Stacks: $(echo $HEALTHY_STACKS | tr ' ' ',' | sed 's/^,//' | sed 's/,/, /g')"
            [ -n "$DEGRADED_STACKS" ] && echo "⚠️ Degraded Stacks: $(echo $DEGRADED_STACKS | tr ' ' ',' | sed 's/^,//' | sed 's/,/, /g')"
            [ -n "$FAILED_STACKS" ] && echo "❌ Failed Stacks: $(echo $FAILED_STACKS | tr ' ' ',' | sed 's/^,//' | sed 's/,/, /g')"

            echo ""
            echo "📋 Detailed Health Check Results:"
            echo "════════════════════════════════════════════════════════════════"
            for STACK in $STACKS; do
              if [ -f "/tmp/health_${STACK}.log" ]; then
                echo ""
                echo "🔸 STACK: $STACK"
                echo "────────────────────────────────────────────────────────────────"
                cat "/tmp/health_${STACK}.log"
                echo "────────────────────────────────────────────────────────────────"
              else
                echo ""
                echo "🔸 STACK: $STACK"
                echo "────────────────────────────────────────────────────────────────"
                echo "⚠️  No health check log found for $STACK"
                echo "────────────────────────────────────────────────────────────────"
              fi
            done
            echo "════════════════════════════════════════════════════════════════"

            # Output structured results for GitHub Actions
            echo "GITHUB_OUTPUT_START"
            echo "healthy_stacks=$(echo $HEALTHY_STACKS | tr ' ' ',' | sed 's/^,//' | sed 's/,/, /g')"
            echo "degraded_stacks=$(echo $DEGRADED_STACKS | tr ' ' ',' | sed 's/^,//' | sed 's/,/, /g')"
            echo "failed_stacks=$(echo $FAILED_STACKS | tr ' ' ',' | sed 's/^,//' | sed 's/,/, /g')"
            echo "total_containers=$TOTAL_CONTAINERS"
            echo "running_containers=$RUNNING_CONTAINERS"
            echo "success_rate=$(( RUNNING_CONTAINERS * 100 / TOTAL_CONTAINERS ))"
            echo "GITHUB_OUTPUT_END"

            # Determine final health status
            if [ -n "$FAILED_STACKS" ]; then
              echo ""
              echo "💥 Health check failed - some stacks are not running"
              exit 1
            elif [ -n "$DEGRADED_STACKS" ]; then
              echo ""
              echo "⚠️ Health check passed with warnings - some services degraded"
              exit 0
            else
              echo ""
              echo "🎉 All services are fully healthy!"
              exit 0
            fi
          EOF
          )

          # Extract health outputs from structured result
          echo "$HEALTH_RESULT"

          # Parse outputs without temporary files
          if echo "$HEALTH_RESULT" | grep -q "GITHUB_OUTPUT_START"; then
            echo "$HEALTH_RESULT" | sed -n '/GITHUB_OUTPUT_START/,/GITHUB_OUTPUT_END/p' | grep -E "^(healthy_stacks|degraded_stacks|failed_stacks|total_containers|running_containers|success_rate)=" >> $GITHUB_OUTPUT
          else
            # Fallback outputs if parsing fails
            echo "healthy_stacks=" >> $GITHUB_OUTPUT
            echo "degraded_stacks=" >> $GITHUB_OUTPUT
            echo "failed_stacks=" >> $GITHUB_OUTPUT
            echo "total_containers=0" >> $GITHUB_OUTPUT
            echo "running_containers=0" >> $GITHUB_OUTPUT
            echo "success_rate=0" >> $GITHUB_OUTPUT
          fi

          echo "::endgroup::"

      - name: Cleanup unused images
        id: cleanup
        if: steps.backup.outputs.deployment_needed == 'true' && steps.deploy.outcome == 'success' && steps.health.outcome == 'success'
        continue-on-error: true
        run: |
          echo "::group::Cleaning up unused Docker images"
          ssh -o "StrictHostKeyChecking no" ${{ secrets.SSH_USER }}@${{ secrets.SSH_HOST }} << EOF
            echo "🧹 Cleaning up unused Docker images..."
            docker image prune -af
            echo "✅ Cleanup completed"
          EOF
          echo "::endgroup::"

      - name: Rollback to Previous Version
        id: rollback
        if: steps.backup.outputs.deployment_needed == 'true' && (steps.deploy.outcome == 'failure' || steps.health.outcome == 'failure')
        continue-on-error: true
        run: |
          echo "::group::Rolling back to previous deployment"
          echo "🔄 **INITIATING ROLLBACK**"
          echo "Previous SHA: ${{ steps.backup.outputs.previous_sha }}"
          echo "Failed SHA: ${{ inputs.target-ref }}"

          # Parse inputs outside SSH context
          HAS_DOCKGE="${{ inputs.has-dockge }}"
          PREVIOUS_SHA="${{ steps.backup.outputs.previous_sha }}"
          COMPOSE_ARGS="${{ inputs.args || '' }}"

          ssh -o "StrictHostKeyChecking no" ${{ secrets.SSH_USER }}@${{ secrets.SSH_HOST }} /bin/bash -s "$HAS_DOCKGE" "$PREVIOUS_SHA" "$COMPOSE_ARGS" << 'EOF'
            set -e

            # Get arguments passed to script (excluding sensitive OP_TOKEN)
            HAS_DOCKGE="$1"
            PREVIOUS_SHA="$2"
            COMPOSE_ARGS="$3"

            # Set OP_TOKEN via environment (passed separately)
            export OP_SERVICE_ACCOUNT_TOKEN="${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}"

            echo "🔄 Rolling back to $PREVIOUS_SHA..."
            git -C /opt/compose/ fetch
            git -C /opt/compose/ checkout $PREVIOUS_SHA

            # Dynamically discover stacks based on the previous commit's structure
            echo "🔍 Discovering stacks in previous commit..."
            ROLLBACK_STACKS=""
            cd /opt/compose
            for dir in */; do
              if [[ -d "$dir" && (-f "$dir/compose.yml" || -f "$dir/compose.yaml") ]]; then
                STACK_NAME=$(basename "$dir")
                ROLLBACK_STACKS="$ROLLBACK_STACKS $STACK_NAME"
                echo "  Found stack: $STACK_NAME"
              fi
            done

            if [ -z "$ROLLBACK_STACKS" ]; then
              echo "⚠️ No stacks found in previous commit - rollback cannot proceed"
              exit 1
            fi

            echo "📋 Stacks to rollback:$ROLLBACK_STACKS"

            # Deploy Dockge first if needed
            if [ "$HAS_DOCKGE" = "true" ]; then
              echo "🔄 Rolling back Dockge..."
              cd /opt/dockge
              op run --env-file=/opt/compose/compose.env -- docker compose pull
              op run --env-file=/opt/compose/compose.env -- docker compose up -d --remove-orphans $COMPOSE_ARGS
              echo "✅ Dockge rolled back successfully"
            fi

            # Function to rollback a single stack (same pattern as deploy)
            rollback_stack() {
              local STACK=$1
              local LOGFILE="/tmp/rollback_${STACK}.log"
              {
                echo "🔄 Rolling back $STACK..."
                cd /opt/compose/$STACK

                echo "  Pulling images for $STACK..."
                if ! op run --env-file=/opt/compose/compose.env -- docker compose pull; then
                  echo "❌ Failed to pull images for $STACK during rollback"
                  return 1
                fi

                echo "  Starting services for $STACK..."
                if ! op run --env-file=/opt/compose/compose.env -- docker compose up -d --remove-orphans $COMPOSE_ARGS; then
                  echo "❌ Failed to start services for $STACK during rollback"
                  return 1
                fi

                echo "✅ $STACK rolled back successfully"
                return 0
              } > "$LOGFILE" 2>&1

              return $?
            }

            # Cleanup function for rollback logs
            cleanup_rollback_logs() {
              for STACK in $ROLLBACK_STACKS; do
                rm -f "/tmp/rollback_${STACK}.log" 2>/dev/null
              done
            }

            # Set trap for cleanup on exit
            trap cleanup_rollback_logs EXIT

            # Start all rollback deployments in parallel
            echo "🔄 Starting parallel rollback of all stacks..."
            ROLLBACK_PIDS=""

            for STACK in $ROLLBACK_STACKS; do
              echo "🔄 Rolling back $STACK..."
              rollback_stack "$STACK" &
              ROLLBACK_PIDS="$ROLLBACK_PIDS $!"
              echo "Started rollback of $STACK (PID: $!)"
            done

            # Wait for all rollback deployments and collect results
            echo "⏳ Waiting for all rollbacks to complete..."
            FAILED_ROLLBACKS=""

            # Wait for all background jobs
            wait

            # Check rollback results from log files
            ROLLED_BACK_STACKS=""
            for STACK in $ROLLBACK_STACKS; do
              if [ -f "/tmp/rollback_${STACK}.log" ]; then
                ROLLED_BACK_STACKS="$ROLLED_BACK_STACKS $STACK"
                if grep -q "❌.*$STACK.*rollback" "/tmp/rollback_${STACK}.log"; then
                  FAILED_ROLLBACKS="$FAILED_ROLLBACKS $STACK"
                fi
              fi
            done

            # Display all rollback logs
            echo ""
            echo "📋 Rollback Results:"
            echo "════════════════════════════════════════════════════════════════"
            for STACK in $ROLLBACK_STACKS; do
              if [ -f "/tmp/rollback_${STACK}.log" ]; then
                echo ""
                echo "🔸 ROLLBACK STACK: $STACK"
                echo "────────────────────────────────────────────────────────────────"
                cat "/tmp/rollback_${STACK}.log"
                echo "────────────────────────────────────────────────────────────────"
              else
                echo ""
                echo "🔸 ROLLBACK STACK: $STACK"
                echo "────────────────────────────────────────────────────────────────"
                echo "⚠️  No rollback log found for $STACK"
                echo "────────────────────────────────────────────────────────────────"
              fi
            done
            echo "════════════════════════════════════════════════════════════════"

            # Check if any rollbacks failed
            if [ -z "$ROLLBACK_STACKS" ]; then
              echo "💥 No stacks to rollback - ROLLBACK_STACKS variable is empty!"
              exit 1
            elif [ -z "$ROLLED_BACK_STACKS" ]; then
              echo "💥 No stacks were actually rolled back - check stack discovery!"
              exit 1
            elif [ -n "$FAILED_ROLLBACKS" ]; then
              echo "💥 Rollbacks failed for:$FAILED_ROLLBACKS"
              exit 1
            fi

            echo "🎉 All stacks rolled back successfully!"
          EOF
          echo "::endgroup::"

      - name: Cleanup SSH connections
        if: always()
        run: |
          # Close SSH connection multiplexing
          echo "🧹 Cleaning up SSH connections..."
          ssh -o "StrictHostKeyChecking no" deployment-server -O exit 2>/dev/null || true

          # Clean up SSH control sockets
          rm -f ~/.ssh/sockets/* 2>/dev/null || true

          echo "✅ SSH cleanup completed"

      - name: Report Deployment Status
        if: always()
        run: |
          echo "::group::Deployment Summary"

          # Parse stacks from JSON input and create display list
          STACK_LIST="${{ join(fromJson(inputs.stacks), ', ') }}"
          if [ "${{ inputs.has-dockge }}" = "true" ]; then
            STACK_LIST="dockge, $STACK_LIST"
          fi

          if [ "${{ steps.backup.outputs.deployment_needed }}" != "true" ]; then
            echo "ℹ️ **NO DEPLOYMENT NEEDED**"
            echo "✅ Repository already at target commit"
            echo "📋 Target stacks: $STACK_LIST"
            echo "🔄 SHA: ${{ inputs.target-ref }}"
          elif [ "${{ inputs.force-deploy }}" = "true" ] && [ "${{ steps.deploy.outcome }}" == "success" ] && [ "${{ steps.health.outcome }}" == "success" ]; then
            echo "🔄 **FORCE DEPLOYMENT SUCCESSFUL**"
            echo "✅ All stacks force-deployed and healthy"
            echo "📋 Deployed stacks: $STACK_LIST"
            echo "🔄 SHA: ${{ inputs.target-ref }}"
            if [ "${{ steps.cleanup.outcome }}" == "success" ]; then
              echo "🧹 Cleanup completed successfully"
            fi
          elif [ "${{ steps.deploy.outcome }}" == "success" ] && [ "${{ steps.health.outcome }}" == "success" ]; then
            echo "🎉 **DEPLOYMENT SUCCESSFUL**"
            echo "✅ All stacks deployed and healthy"
            echo "📋 Deployed stacks: $STACK_LIST"
            echo "🔄 SHA: ${{ inputs.target-ref }}"
            if [ "${{ steps.cleanup.outcome }}" == "success" ]; then
              echo "🧹 Cleanup completed successfully"
            fi
          else
            echo "💥 **DEPLOYMENT FAILED**"
            echo "❌ Deploy status: ${{ steps.deploy.outcome }}"
            echo "❌ Health check status: ${{ steps.health.outcome }}"
            if [ "${{ steps.rollback.outcome }}" == "success" ]; then
              echo "🔄 Rollback completed successfully"
            else
              echo "❌ Rollback status: ${{ steps.rollback.outcome }}"
            fi
            exit 1
          fi
          echo "::endgroup::"

  notify:
    name: Discord Notification
    runs-on: ubuntu-24.04
    needs: [deploy]
    if: always()
    steps:
      - name: Configure 1Password Service Account
        uses: 1password/load-secrets-action/configure@13f58eec611f8e5db52ec16247f58c508398f3e6  # v3.0.0
        with:
          service-account-token: ${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}

      - name: Get commit message
        id: commit-msg
        run: |
          COMMIT_MSG=$(curl -s -H "Authorization: token ${{ github.token }}" \
            "https://api.github.com/repos/${{ github.repository }}/commits/${{ inputs.target-ref }}" \
            | jq -r '.commit.message // "No commit message available"' \
            | head -1)
          SHORT_SHA="${{ inputs.target-ref }}"
          SHORT_SHA="${SHORT_SHA:0:7}"
          echo "message=$COMMIT_MSG" >> $GITHUB_OUTPUT
          echo "short-sha=$SHORT_SHA" >> $GITHUB_OUTPUT

      - name: Load Discord webhook
        id: op-load-discord
        uses: 1password/load-secrets-action@13f58eec611f8e5db52ec16247f58c508398f3e6  # v3.0.0
        with:
          unset-previous: true
        env:
          DISCORD_WEBHOOK: ${{ inputs.webhook-url }}

      - name: Send Discord notification
        uses: sarisia/actions-status-discord@11a0bfe3b50977e38aa2bd4a4ebd296415e83c19  # v1.15.4
        with:
          webhook: ${{ steps.op-load-discord.outputs.DISCORD_WEBHOOK }}
          status: ${{ needs.deploy.outputs.deployment_needed != 'true' && 'success' || needs.deploy.outputs.deploy_status == 'success' && needs.deploy.outputs.health_status == 'success' && 'success' || 'failure' }}
          title: "🚀 ${{ inputs.repo-name }} Deployment"
          description: |
            ${{ needs.deploy.outputs.deployment_needed != 'true' && 'ℹ️ **NO DEPLOYMENT NEEDED** - Repository already at target commit' || inputs.force-deploy == true && needs.deploy.outputs.deploy_status == 'success' && needs.deploy.outputs.health_status == 'success' && '🔄 **FORCE DEPLOYMENT SUCCESSFUL**' || needs.deploy.outputs.deploy_status == 'success' && needs.deploy.outputs.health_status == 'success' && '✅ **DEPLOYMENT SUCCESSFUL**' || needs.deploy.outputs.rollback_status == 'success' && '🔄 **DEPLOYMENT FAILED - ROLLED BACK**' || '❌ **DEPLOYMENT FAILED**' }}

            ${{ needs.deploy.outputs.deployment_needed == 'true' && format('**Services:** {0}/{1} running ({2}%)', needs.deploy.outputs.running_containers || '?', needs.deploy.outputs.total_containers || '?', needs.deploy.outputs.success_rate || '?') || '**Target Stacks:** Ready for deployment when needed' }}
            ${{ needs.deploy.outputs.healthy_stacks != '' && format('✅ Healthy: {0}', needs.deploy.outputs.healthy_stacks) || '' }}${{ needs.deploy.outputs.degraded_stacks != '' && format('⚠️ Degraded: {0}', needs.deploy.outputs.degraded_stacks) || '' }}${{ needs.deploy.outputs.failed_stacks != '' && format('❌ Failed: {0}', needs.deploy.outputs.failed_stacks) || '' }}

            ${{ needs.deploy.outputs.deployment_needed == 'true' && format('**Pipeline:** Deploy {0} → Health {1} → Cleanup {2}{3}', needs.deploy.outputs.deploy_status == 'success' && '✅' || '❌', needs.deploy.outputs.health_status == 'success' && '✅' || needs.deploy.outputs.health_status == 'skipped' && '⏭️' || '❌', needs.deploy.outputs.cleanup_status == 'success' && '✅' || needs.deploy.outputs.cleanup_status == 'skipped' && '⏭️' || '❌', needs.deploy.outputs.rollback_status != 'skipped' && format(' → Rollback {0}', needs.deploy.outputs.rollback_status == 'success' && '✅' || '❌') || '') || '**Status:** No changes detected' }}

            ${{ github.event_name == 'workflow_dispatch' && '🔧 **Manual Trigger**' || format('📝 **Commit:** `{0}` - {1}', steps.commit-msg.outputs.short-sha, steps.commit-msg.outputs.message) }}
          color: ${{ needs.deploy.outputs.deployment_needed != 'true' && 0x808080 || needs.deploy.outputs.deploy_status == 'success' && needs.deploy.outputs.health_status == 'success' && 0x00ff00 || needs.deploy.outputs.degraded_stacks != '' && 0xff9900 || 0xff0000 }}
          username: "GitHub Actions"
          avatar_url: "https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"

      - name: Unload Discord webhook
        uses: 1password/load-secrets-action@13f58eec611f8e5db52ec16247f58c508398f3e6  # v3.0.0
        with:
          unset-previous: true
